{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Initiate and Configure Spark**","metadata":{"id":"bnyhiA51pzxv"}},{"cell_type":"code","source":"!pip3 install pyspark","metadata":{"id":"soB6_OC8wQpC","outputId":"fb2ef3bf-c594-4685-940d-c42a7c5e32b1","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:23:16.025450Z","iopub.execute_input":"2025-01-10T20:23:16.025935Z","iopub.status.idle":"2025-01-10T20:23:57.903271Z","shell.execute_reply.started":"2025-01-10T20:23:16.025856Z","shell.execute_reply":"2025-01-10T20:23:57.902054Z"}},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849765 sha256=b19bea3382a1fb764932370fa63b583a908b719a60defa66ba5cc2703171d7f1\n  Stored in directory: /root/.cache/pip/wheels/d9/1c/98/31e395a42d1735d18d42124971ecbbade844b50bb9845b6f4a\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName(\"movieDataset\") \\\n                    .master(\"local[*]\") \\\n                    .config(\"spark.executor.memory\", \"10g\") \\\n                    .config(\"spark.driver.memory\", \"10g\") \\\n                    .config(\"spark.executor.cores\", \"2\") \\\n                    .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n                    .getOrCreate()\nspark","metadata":{"id":"PWEOwujOwV39","outputId":"6e898ef3-3928-4622-c1ff-bd96f27d1d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:23:57.905020Z","iopub.execute_input":"2025-01-10T20:23:57.905414Z","iopub.status.idle":"2025-01-10T20:24:05.132565Z","shell.execute_reply.started":"2025-01-10T20:23:57.905382Z","shell.execute_reply":"2025-01-10T20:24:05.131260Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x78f3f81f2860>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://3cdea1c96b96:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>movieDataset</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"nCLDNM7IyH4O","outputId":"e2261f43-5795-42a6-d5ae-ec5fb897ad60","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.135470Z","iopub.execute_input":"2025-01-10T20:24:05.136260Z","iopub.status.idle":"2025-01-10T20:24:05.261214Z","shell.execute_reply.started":"2025-01-10T20:24:05.136215Z","shell.execute_reply":"2025-01-10T20:24:05.257135Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."],"ename":"NotImplementedError","evalue":"Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"---\n# **Task 1 - Data Loading and Preprocessing**\n---","metadata":{"id":"09GqEutkqmIL"}},{"cell_type":"code","source":"# Load the data CSV file from google drive\n\ndf = spark.read.csv(\"/content/drive/MyDrive/ML&Bigdata/movies.csv\", header=True, inferSchema=True)","metadata":{"id":"FrJMVh9QyMsR","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.262206Z","iopub.status.idle":"2025-01-10T20:24:05.262855Z","shell.execute_reply":"2025-01-10T20:24:05.262510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the dataset\n\ndf.show()","metadata":{"id":"2at9-geayPSi","outputId":"05908ba5-6d02-4ec8-c727-ecd795992be8","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.263679Z","iopub.status.idle":"2025-01-10T20:24:05.264168Z","shell.execute_reply":"2025-01-10T20:24:05.263981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View number of non null values and columns in the data frame\n\nprint((df.count(), len(df.columns)))","metadata":{"id":"pPDHakFfya-H","outputId":"c4a71d53-64ce-417f-d68e-61eea37fc836","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.265779Z","iopub.status.idle":"2025-01-10T20:24:05.266276Z","shell.execute_reply":"2025-01-10T20:24:05.266075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the names of columns of the dataframe\n\ndf.columns","metadata":{"id":"MPbGkNftydGI","outputId":"2df799f1-64c8-4198-f8d2-ffca90012180","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.267368Z","iopub.status.idle":"2025-01-10T20:24:05.267824Z","shell.execute_reply":"2025-01-10T20:24:05.267619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View data types in the dataset\n\ndf.printSchema()","metadata":{"id":"wm3eAvUjye7z","outputId":"f2dc76da-2413-40a3-a24d-6c7b92ed603f","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.268656Z","iopub.status.idle":"2025-01-10T20:24:05.269119Z","shell.execute_reply":"2025-01-10T20:24:05.268931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupBy('genre').count().orderBy('count').show(10,False)","metadata":{"id":"h0EK1BVvyiJe","outputId":"95627bf0-7c0f-40b5-b603-c8d90c157bfc","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.269953Z","iopub.status.idle":"2025-01-10T20:24:05.270393Z","shell.execute_reply":"2025-01-10T20:24:05.270206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Missing Values","metadata":{"id":"hrfZhruYns5S"}},{"cell_type":"code","source":"# As there are a total of 2593384 and roughly 2450 have missing values, this is a small percentage, less than 2% of missing data which should not largely impact the findings if deleted\n\n\nfrom pyspark.sql.functions import col, sum\n\nmissing_values = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n\nmissing_values.show()","metadata":{"id":"fG3S_r6WyrIn","outputId":"d1223c71-3033-4bdc-b24f-51244f63abf5","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.271115Z","iopub.status.idle":"2025-01-10T20:24:05.271520Z","shell.execute_reply":"2025-01-10T20:24:05.271352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Delete missing values within the dataset\n\ndf_cleaned = df.dropna()\ndf_cleaned.count()","metadata":{"id":"T6ygJzM8ytS9","outputId":"96bdcc0a-06ea-4970-bf15-c32b4fe45fb7","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.272634Z","iopub.status.idle":"2025-01-10T20:24:05.273100Z","shell.execute_reply":"2025-01-10T20:24:05.272900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check that all rows with missing values have been removed\n\nmissing_values = df_cleaned.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns])\n\nmissing_values.show()","metadata":{"id":"oJqX6O-jyu7N","outputId":"bfab7fad-f9d7-4220-93d7-bb4a5d4b6c99","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.274142Z","iopub.status.idle":"2025-01-10T20:24:05.274554Z","shell.execute_reply":"2025-01-10T20:24:05.274369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop unecessary columns - the directors_name instead of directors id and stars_name instead of stars_id. They both represent the same thing.\n\ndf2=df_cleaned.drop('id', 'directors_name', 'stars_name')\ndf2.show(10)","metadata":{"id":"8Jiuklugyw09","outputId":"a1a8d256-50d0-48d0-a2be-a6c31e26a329","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.275335Z","iopub.status.idle":"2025-01-10T20:24:05.275775Z","shell.execute_reply":"2025-01-10T20:24:05.275579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Columns 'rating','votes' and 'gross_income' to numerical format\n\nfrom pyspark.sql.functions import col\ndf2 = df2.withColumn(\"ranking\", col(\"rating\").cast(\"double\"))\ndf2 = df2.withColumn(\"public_vote\", col(\"votes\").cast(\"double\"))\ndf2 = df2.withColumn(\"income\", col(\"gross_income\").cast(\"double\"))","metadata":{"id":"aiHO7nJ7yzKx","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.276447Z","iopub.status.idle":"2025-01-10T20:24:05.276902Z","shell.execute_reply":"2025-01-10T20:24:05.276676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View data types to check if the new columns with their new data types have been created\n\ndf2.printSchema()","metadata":{"id":"CnQYL0dzy1nA","outputId":"1a02a0c0-9ece-4412-aafa-85693329168d","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.277799Z","iopub.status.idle":"2025-01-10T20:24:05.278246Z","shell.execute_reply":"2025-01-10T20:24:05.278060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df3=df2.select('name','description','year','certificate','duration','genre','directors_id','stars_id','income','public_vote','ranking')\ndf3.show()","metadata":{"id":"cM0n7vYny30w","outputId":"374dd5bd-eff1-4931-d86c-758f2f6e1738","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.279014Z","iopub.status.idle":"2025-01-10T20:24:05.279436Z","shell.execute_reply":"2025-01-10T20:24:05.279252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"String Indexer","metadata":{"id":"NOQDi0IHy6E2"}},{"cell_type":"code","source":"# Convert all categorical data columns into numerical format\n\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol = 'certificate', outputCol = 'certified')\nindexer2 = StringIndexer(inputCol = 'genre', outputCol = 'movie_genre')\nindexer3 = StringIndexer(inputCol = 'duration', outputCol = 'movie_length')\nindexer4 = StringIndexer(inputCol = 'directors_id', outputCol = 'directors')\nindexer5 = StringIndexer(inputCol = 'stars_id', outputCol = 'stars')\nindexer6 = StringIndexer(inputCol = 'year', outputCol = 'date')\ndf3 = indexer.fit(df3).transform(df3)\ndf3 = indexer2.fit(df3).transform(df3)\ndf3 = indexer3.fit(df3).transform(df3)\ndf3 = indexer4.fit(df3).transform(df3)\ndf3 = indexer5.fit(df3).transform(df3)\ndf3 = indexer6.fit(df3).transform(df3)\ndf3.show(10)","metadata":{"id":"V-5VzUW-zBWv","outputId":"b0e385a6-6cfa-46a4-8794-a040c0cac994","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.280285Z","iopub.status.idle":"2025-01-10T20:24:05.280714Z","shell.execute_reply":"2025-01-10T20:24:05.280526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select all string indexed columns into a new dataframe\n\ndf4=df3.select('name','description','date','certified','movie_length','movie_genre','directors','stars','income','public_vote','ranking')\ndf4.show()","metadata":{"id":"fyPe-6sRzFob","outputId":"08dedc8e-0f47-47c8-e93c-edd3d2c4f03a","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.281765Z","iopub.status.idle":"2025-01-10T20:24:05.282216Z","shell.execute_reply":"2025-01-10T20:24:05.282028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Starting with tokenization of the two text columns to split the text into individual words/tokens\n\nfrom pyspark.ml.feature import Tokenizer\ntokenization=Tokenizer(inputCol='name',outputCol='title')\ntokenization2=Tokenizer(inputCol='description',outputCol='summary')\ntokenized_df2 = tokenization.transform(df4)\ntokenized_df2 = tokenization2.transform(tokenized_df2)\ntokenized_df2.show(4,False)","metadata":{"id":"x9n94JNFzGJK","outputId":"866e2a3f-edfb-45b1-ccca-e82595b8fd07","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.283311Z","iopub.status.idle":"2025-01-10T20:24:05.283753Z","shell.execute_reply":"2025-01-10T20:24:05.283566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This has now removed stop words such as the, and, in from both 'summary' and 'title' columns and outputted into new columns 'refined_summary' and 'refined_title'\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nstopword_removal=StopWordsRemover(inputCol='title',outputCol='refined_title')\nstopword_removal2=StopWordsRemover(inputCol='summary',outputCol='refined_summary')\nrefined_df2=stopword_removal.transform(tokenized_df2)\nrefined_df2=stopword_removal2.transform(refined_df2)\n\nrefined_df2.show(10,False)","metadata":{"id":"zChVa2ZizKMd","outputId":"89adaa0a-aebd-448b-fd78-4b021fb3c684","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.284487Z","iopub.status.idle":"2025-01-10T20:24:05.284935Z","shell.execute_reply":"2025-01-10T20:24:05.284732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","metadata":{"id":"skBrPtJFzS7E","outputId":"7d8125df-b88d-4dad-cb7c-e65784f4d483","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.285734Z","iopub.status.idle":"2025-01-10T20:24:05.286153Z","shell.execute_reply":"2025-01-10T20:24:05.285981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tidy up the text by using lemmatizer to reduce words to their base form\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef lemmatize_text(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n    return lemmas\n\n\nlemmatize_udf = udf(lemmatize_text, ArrayType(StringType()))\n\n# Apply lemmatization using the User Defined Function\nrefined_df2 = refined_df2.withColumn(\"lemmatized_title\", lemmatize_udf(\"refined_title\"))\nrefined_df2 = refined_df2.withColumn(\"lemmatized_summary\", lemmatize_udf(\"refined_summary\"))\n\n# View the resulting Dataframe\nrefined_df2.show(truncate=False)\n\n","metadata":{"id":"LMgnqDm-zVo2","outputId":"4bb5288e-01b6-4620-fbb2-bac081aefb23","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.287098Z","iopub.status.idle":"2025-01-10T20:24:05.287524Z","shell.execute_reply":"2025-01-10T20:24:05.287328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use Hashing TF-DF to find out the importance of each word which is then given a number\n\nfrom pyspark.ml.feature import HashingTF,IDF\nhashing_vec2=HashingTF(inputCol='lemmatized_title',outputCol='title_features')\nhashing_vec3=HashingTF(inputCol='lemmatized_summary',outputCol='summary_features')\nhashing_df2=hashing_vec2.transform(refined_df2)\nhashing_df2=hashing_vec3.transform(hashing_df2)\nhashing_df2.select(['lemmatized_title','title_features','lemmatized_summary','summary_features']).show(4,False)","metadata":{"id":"NZ9PCLs0zYPa","outputId":"b7888690-6095-43c4-9789-f54c098b66bc","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.288331Z","iopub.status.idle":"2025-01-10T20:24:05.288784Z","shell.execute_reply":"2025-01-10T20:24:05.288585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ninput_cols=['title_features', 'summary_features','date','public_vote','movie_genre','directors','stars','movie_length','certified','income',]\n\n# Assemble all features together using vector assembler so this can be easily fed into the maching learning model\n\nvec_assembler = VectorAssembler(inputCols = input_cols, outputCol='features')\nfinal_data = vec_assembler.transform(hashing_df2)\nfinal_data.show()","metadata":{"id":"TRfx2I5bzd9y","outputId":"f3cb64c6-bc3f-4b27-8448-4953cb3b3b95","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.289699Z","iopub.status.idle":"2025-01-10T20:24:05.290142Z","shell.execute_reply":"2025-01-10T20:24:05.289956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select final data frame being the features and label column 'ranking'\n\nfinal_data3=final_data.select('features','ranking')\nfinal_data3.show()","metadata":{"id":"toU3sFOZzfzO","outputId":"338d01d4-bfd9-4c4c-c0b4-d1e01501dfea","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.290974Z","iopub.status.idle":"2025-01-10T20:24:05.291399Z","shell.execute_reply":"2025-01-10T20:24:05.291214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adjust the numerical data so that they are centered around zero and have a standard deviation of 1 to limit bias\n\nfrom pyspark.ml.feature import StandardScaler\n\nscaler = StandardScaler(inputCol = 'features', outputCol = 'scaledFeatures')\n\n\nscaler_model = scaler.fit(final_data3)\nfinal_data3 = scaler_model.transform(final_data3)\n\nfinal_data3 = final_data3.select('scaledFeatures', 'ranking')\nfinal_data3.show(5, truncate = False)","metadata":{"id":"kYWK5nJ_zh8W","outputId":"1dba56fa-a07a-49aa-e0f9-c712510e7a96","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.292578Z","iopub.status.idle":"2025-01-10T20:24:05.293032Z","shell.execute_reply":"2025-01-10T20:24:05.292817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training data, test data split","metadata":{"id":"tN06zBnezkt9"}},{"cell_type":"code","source":"# Split data to prepare for training\n\ntrain_data, test_data = final_data3.randomSplit([0.7, 0.3], seed = 42)\ntrain_data.show(5, truncate = False)","metadata":{"id":"n-9HoVfSzkBA","outputId":"b7447001-d1aa-43fc-b7f0-ea3e92913259","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.294256Z","iopub.status.idle":"2025-01-10T20:24:05.294977Z","shell.execute_reply":"2025-01-10T20:24:05.294702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **Task 2 - Model Selection and Implementation**\n---","metadata":{"id":"1jkm_0LBq74T"}},{"cell_type":"code","source":"# Train linear regression model\n\nlr = LinearRegression(labelCol = 'ranking', featuresCol = 'scaledFeatures', predictionCol = 'prediction')\n\nlr_model = lr.fit(train_data)","metadata":{"id":"Ns_DAGy-zsPO","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.295982Z","iopub.status.idle":"2025-01-10T20:24:05.296427Z","shell.execute_reply":"2025-01-10T20:24:05.296236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_predictions = lr_model.transform(test_data)","metadata":{"id":"xlz40HWO4LlH","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.297386Z","iopub.status.idle":"2025-01-10T20:24:05.297838Z","shell.execute_reply":"2025-01-10T20:24:05.297648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show predictions for the model\n\nlr_predictions.select('prediction', 'ranking').show(10, truncate = False)","metadata":{"id":"lJuoQ0SOYOwW","outputId":"0d818961-c84c-4cf2-b1e3-0016fae6c97a","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.298888Z","iopub.status.idle":"2025-01-10T20:24:05.299335Z","shell.execute_reply":"2025-01-10T20:24:05.299136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the coefficients and intecepts\n\ncoefficients = lr_model.coefficients\nintercept = lr_model.intercept","metadata":{"id":"NSvFZYaSZNXD","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.300236Z","iopub.status.idle":"2025-01-10T20:24:05.300727Z","shell.execute_reply":"2025-01-10T20:24:05.300547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **Task 3 - Model Parameter Tuning**\n---","metadata":{"id":"dsClW5k-rId8"}},{"cell_type":"code","source":"# Train the Lasso model\nlasso = LinearRegression(labelCol = 'ranking', featuresCol = 'scaledFeatures',\n                         predictionCol = 'prediction', elasticNetParam = 1.0, regParam = 0.15)\nlasso_model = lasso.fit(train_data)\nlasso_predictions = lasso_model.transform(test_data)\n\n# Train the Ridge model\nridge = LinearRegression(labelCol = 'ranking', featuresCol = 'scaledFeatures',\n                         predictionCol = 'prediction', elasticNetParam = 0.0, regParam = 0.15)\nridge_model = ridge.fit(train_data)\nridge_predictions = ridge_model.transform(test_data)","metadata":{"id":"U_xCE9SpaYxA","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.301542Z","iopub.status.idle":"2025-01-10T20:24:05.302027Z","shell.execute_reply":"2025-01-10T20:24:05.301798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print lasso model predictions\n\nlasso_predictions.select('prediction', 'ranking').show(10, truncate = False)","metadata":{"id":"iNlUjVLsPHW8","outputId":"43712942-26aa-472e-94ac-f882d60bf939","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.303053Z","iopub.status.idle":"2025-01-10T20:24:05.303368Z","shell.execute_reply":"2025-01-10T20:24:05.303242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **Task 4 - Model Evaluation and Accuracy Calculation**\n---","metadata":{"id":"DtUxLhZ8rYMI"}},{"cell_type":"code","source":"# Measure the average squared difference between predicted values and actual values\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator_mse = RegressionEvaluator(labelCol = 'ranking', predictionCol = 'prediction', metricName = 'mse')\n# calculate MSE\nmse1 = evaluator_mse.evaluate(lr_predictions)\nmse2 = evaluator_mse.evaluate(lasso_predictions)\nmse3 = evaluator_mse.evaluate(ridge_predictions)\n","metadata":{"id":"Cz6djrQbQ4Lf","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.306450Z","iopub.status.idle":"2025-01-10T20:24:05.306947Z","shell.execute_reply":"2025-01-10T20:24:05.306736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Measure the square root of the MSE\n\nevaluator_rmse = RegressionEvaluator(labelCol = 'ranking', predictionCol = 'prediction', metricName = 'rmse')\n# calculate RMSE\nrmse1 = evaluator_rmse.evaluate(lr_predictions)\nrmse2 = evaluator_rmse.evaluate(lasso_predictions)\nrmse3 = evaluator_rmse.evaluate(ridge_predictions)\n\n# Measure how well the independant variables predict the actual data\n\nevaluator_r2 = RegressionEvaluator(labelCol = 'ranking', predictionCol = 'prediction', metricName = 'r2')\n# calculate R_squared\nr2_score1 = evaluator_r2.evaluate(lr_predictions)\nr2_score2 = evaluator_r2.evaluate(lasso_predictions)\nr2_score3 = evaluator_r2.evaluate(ridge_predictions)","metadata":{"id":"ntj6Bk9bRfYx","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.310360Z","iopub.status.idle":"2025-01-10T20:24:05.310831Z","shell.execute_reply":"2025-01-10T20:24:05.310647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print full evaluation metrics\nprint('Regression - MSE: ', mse1, ', RMSE: ', rmse1, ', R^2: ', r2_score1)\nprint('Lasso - MSE: ', mse2, ', RMSE: ', rmse2, ', R^2: ', r2_score2)\nprint('Ridge - MSE: ', mse3, ', RMSE: ', rmse3, ', R^2: ', r2_score3)","metadata":{"id":"4viKw6oitWC4","outputId":"9b5905ef-9d9a-4b71-ca5a-5275f6c23e0d","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.311912Z","iopub.status.idle":"2025-01-10T20:24:05.312410Z","shell.execute_reply":"2025-01-10T20:24:05.312241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **Task 5 - Results Visualization or Printing**\n---","metadata":{"id":"gesg7c4nre6u"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nmse = [mse1, mse2, mse3]\nrmse = [rmse1, rmse2, rmse3]\nr2_score = [r2_score1, r2_score2, r2_score3]\n\npositions = np.arange(len(mse))\nbar_width = 0.2\n\nplt.bar(positions - bar_width, mse, width = bar_width, label = 'MSE')\nplt.bar(positions, rmse, width = bar_width, label = 'RMSE')\nplt.bar(positions + bar_width, r2_score, width = bar_width, label = 'R2_Score')\n\n# add titles\nplt.xlabel('Model')\nplt.ylabel('scores')\nplt.title('Comparison of Regression Metrics')\n\n# add legend\nplt.legend()\nplt.xticks(positions, ['Regression', 'Lasso', 'Ridge'])\nplt.show()","metadata":{"id":"mcJp0eIvtgAk","outputId":"45635c2e-362f-459a-f2b1-f1a9f8d30d03","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T20:24:05.313134Z","iopub.status.idle":"2025-01-10T20:24:05.313529Z","shell.execute_reply":"2025-01-10T20:24:05.313358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Regression model:**\n\n- The regression model has a high MSE of 3.162624008531466 and RMSE of 1.7783767903713392 meaning the predicted values are roughly 1.7783 from the actual values.\n\n- R-squared at 0.0376 suggests that only a small percentage, roughly 4% of the variability in movie ratings is explained by the features. This indicates that the linear regression model may not be capturing the complex relationships between the features and movie ratings well.\n\n- This suggests that the features used, have a very weak relationship with the target variable (movie ratings).\n\n\n**Lasso model:**\n\n- The Lasso model has a lower MSE 2.3875812451389478 and RMSE 1.5451800041221566 which shows that this models predictions are closer to the actual values - This improvement suggests that Lasso's feature selection helps eliminate irrelevant or less impactful features, making the model more effective at explaining the variance in movie ratings\n\n**Ridge model:**\n\n- The Ridge model also has lower MSE 2.4118655262705526 and RMSE  1.5530181989502096 - This indicates that both regularised models are effective in addressing overfitting and improving prediction accuracy compared to the baseline linear regression.\n\n\n","metadata":{"id":"oQYwQPucvLUt"}},{"cell_type":"markdown","source":"","metadata":{"id":"nPsoYUEnry2-"}}]}